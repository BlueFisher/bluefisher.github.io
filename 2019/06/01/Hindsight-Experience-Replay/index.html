<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-logo.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32-logo.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16-logo.png">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"bluefisher.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="转载自 Keavnn&#39;s Blog https:&#x2F;&#x2F;stepneverstop.github.io&#x2F;Hindsight-Experience-Replay.html 本文介绍了一个“事后诸葛亮”的经验池机制，简称为HER，它可以很好地应用于稀疏奖励和二分奖励的问题中，不需要复杂的奖励函数工程设计。 推荐：  稀疏奖励问题的一种解决方案 通俗易懂">
<meta property="og:type" content="article">
<meta property="og:title" content="Hindsight Experience Replay">
<meta property="og:url" content="https://bluefisher.github.io/2019/06/01/Hindsight-Experience-Replay/index.html">
<meta property="og:site_name" content="Fisher&#39;s Blog">
<meta property="og:description" content="转载自 Keavnn&#39;s Blog https:&#x2F;&#x2F;stepneverstop.github.io&#x2F;Hindsight-Experience-Replay.html 本文介绍了一个“事后诸葛亮”的经验池机制，简称为HER，它可以很好地应用于稀疏奖励和二分奖励的问题中，不需要复杂的奖励函数工程设计。 推荐：  稀疏奖励问题的一种解决方案 通俗易懂">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://bluefisher.github.io/images/2019-06-01-Hindsight-Experience-Replay/hindsight.png">
<meta property="og:image" content="https://bluefisher.github.io/images/2019-06-01-Hindsight-Experience-Replay/Her.png">
<meta property="og:image" content="https://bluefisher.github.io/images/2019-06-01-Hindsight-Experience-Replay/pseudo.png">
<meta property="og:image" content="https://bluefisher.github.io/images/2019-06-01-Hindsight-Experience-Replay/tasks.png">
<meta property="og:image" content="https://bluefisher.github.io/images/2019-06-01-Hindsight-Experience-Replay/finalvsfuture.png">
<meta property="og:image" content="https://bluefisher.github.io/images/2019-06-01-Hindsight-Experience-Replay/singlegoal.png">
<meta property="og:image" content="https://bluefisher.github.io/images/2019-06-01-Hindsight-Experience-Replay/rewardshape.png">
<meta property="og:image" content="https://bluefisher.github.io/images/2019-06-01-Hindsight-Experience-Replay/fourmodel.png">
<meta property="article:published_time" content="2019-06-01T11:52:53.000Z">
<meta property="article:modified_time" content="2019-06-01T12:02:20.000Z">
<meta property="article:author" content="Fisher Chang">
<meta property="article:tag" content="rl">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://bluefisher.github.io/images/2019-06-01-Hindsight-Experience-Replay/hindsight.png">

<link rel="canonical" href="https://bluefisher.github.io/2019/06/01/Hindsight-Experience-Replay/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Hindsight Experience Replay | Fisher's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-124171741-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-124171741-1');
      }
    </script>


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?553ac1673540f93b43866b93cc2d3efe";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Fisher's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Sein heißt Werden<br/>Leben heißt Lernen</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/BlueFisher" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bluefisher.github.io/2019/06/01/Hindsight-Experience-Replay/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars0.githubusercontent.com/u/6104460?v=3&s=200">
      <meta itemprop="name" content="Fisher Chang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Fisher's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Hindsight Experience Replay
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2019-06-01 19:52:53 / 修改时间：20:02:20" itemprop="dateCreated datePublished" datetime="2019-06-01T19:52:53+08:00">2019-06-01</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
            <span id="/2019/06/01/Hindsight-Experience-Replay/" class="post-meta-item leancloud_visitors" data-flag-title="Hindsight Experience Replay" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/06/01/Hindsight-Experience-Replay/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/06/01/Hindsight-Experience-Replay/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>转载自 Keavnn's Blog <a href="https://stepneverstop.github.io/Hindsight-Experience-Replay.html" target="_blank" rel="noopener">https://stepneverstop.github.io/Hindsight-Experience-Replay.html</a></p>
<p>本文介绍了一个“事后诸葛亮”的经验池机制，简称为<strong>HER</strong>，它可以很好地应用于<strong>稀疏奖励</strong>和<strong>二分奖励</strong>的问题中，不需要复杂的奖励函数工程设计。</p>
<p>推荐：</p>
<ul>
<li>稀疏奖励问题的一种解决方案</li>
<li>通俗易懂</li>
</ul>
<a id="more"></a>
<h1 id="简介">简介</h1>
<p>论文地址：<a href="https://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf</a></p>
<blockquote>
<p>Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL).</p>
</blockquote>
<p>强化学习问题中最棘手的问题之一就是稀疏奖励。</p>
<p>本文提出了一个新颖的技术：Hindsight Experience Replay（HER），可以从稀疏、二分的奖励问题中高效采样并进行学习，而且可以应用于<strong>所有的Off-Policy</strong>算法中。</p>
<p><img src="/images/2019-06-01-Hindsight-Experience-Replay/hindsight.png" /></p>
<p>Hindsight意为事后，结合强化学习中序贯决策问题的特性，我们很容易就可以猜想到，“事后”要不然指的是在状态s下执行动作a之后，要不然指的就是当一个episode结束之后。其实，文中对常规经验池的改进也正是运用了这样的含义。</p>
<blockquote>
<p>HER lets an agent learn from undesired outcomes and tackles the problem of sparse rewards in Reinforcement Learning (RL).——Zhao, R., &amp; Tresp, V. (2018). Energy-Based Hindsight Experience Prioritization. <em>CoRL</em>.</p>
</blockquote>
<p>HER使智能体从没达到的结果中去学习，解决了强化学习中稀疏奖励的问题。</p>
<h2 id="二分奖励-binary-reward">二分奖励 binary reward</h2>
<p>简言之，完成目标为一个值，没完成目标为另一个值。如：</p>
<ul>
<li><span class="math inline">\(S_{T}=Goal，r=0​\)</span></li>
<li><span class="math inline">\(S\neq Goal, r=-1. for \ S \in \mathbb{S}​\)</span></li>
</ul>
<h2 id="稀疏奖励-sparse-reward">稀疏奖励 sparse reward</h2>
<p>简言之，完成目标的episode太少或者完成目标的步数太长，导致负奖励的样本数过多</p>
<h1 id="文中精要">文中精要</h1>
<p>在机器人领域，要想使强化学习训练它完美执行某任务，往往需要设计合理的奖励函数，但是设计这样的奖励函数工程师不仅需要懂得强化学习的领域知识，也需要懂得机器人、运动学等领域的知识。而且，有这些知识也未必能设计出很好的奖励函数供智能体进行学习。因此，如果可以从简单的奖励函数（如二分奖励）学习到可完成任务的模型，那就不需要费心设计复杂的奖励函数了。</p>
<p>文中介绍了一个例子来引入HER：</p>
<ul>
<li>名称：bit-flipping environment</li>
<li>状态空间 <span class="math inline">\(\mathcal{S}=\left \{ 0,1 \right \}^{n}\)</span></li>
<li>动作空间 <span class="math inline">\(\mathcal{A}=\left \{ 0,1,\cdots,n-1 \right \}\)</span></li>
<li>规则：对于每个episode，均匀采样长度为<span class="math inline">\(n\)</span>的初始状态 <span class="math inline">\(s_{0}\)</span>（如 <span class="math inline">\(n=5，s_{0}=10101\)</span>）和目标状态 <span class="math inline">\(s_{g}\)</span>，每一步从动作空间中选取一个动作 <span class="math inline">\(a\)</span>，翻转 <span class="math inline">\(s_{0}\)</span> 第 <span class="math inline">\(a\)</span> 个位置的值，如 <span class="math inline">\(a=1\Rightarrow s_{1}=11101\)</span> ，直到回合结束或者翻转后的状态与 <span class="math inline">\(s_{g}\)</span> 相同</li>
<li>奖励函数： <span class="math inline">\(r_{g}(s,a)=-\left [ s \neq g \right ]\)</span> ，即达到目标状态则为0，未达到目标状态则为 -1。这个很容易理解， <span class="math inline">\(s \neq g \Rightarrow true \doteq 1，s = g \Rightarrow false \doteq 0​\)</span></li>
</ul>
<p><em>注：下文如无特殊说明， <span class="math inline">\(g\)</span> 即表示目标状态 <span class="math inline">\(s_{g}\)</span></em></p>
<blockquote>
<p>Standard RL algorithms are bound to fail in this environment for n &gt; 40 because they will never experience any reward other than -1. Notice that using techniques for improving exploration (e.g. VIME (Houthooft et al., 2016), count-based exploration (Ostrovski et al., 2017) or bootstrapped DQN (Osband et al., 2016)) does not help here because the real problem is not in lack of diversity of states being visited, rather it is simply impractical to explore such a large state space.</p>
</blockquote>
<p>当序列长度<span class="math inline">\(n\)</span>大于40时，传统的强化学习算法就算有各种探索机制的加持，也不能学会解决这个问题，因为这个问题完全不是缺乏探索，而是<strong>状态太多，探索不完</strong>，导致奖励极其稀疏，算法根本不知道需要优化的目标在哪里。</p>
<p>为了解决这个问题，作者指出了两个思路：</p>
<ol type="1">
<li>使用shaped reward（简言之，将reward设计成某些变量的函数，如 <span class="math inline">\(r_{g}(s,a)=-\left || s-g \right ||^{2}\)</span> ，即奖励函数为当前状态与目标状态的欧氏距离的负数），将训练的算法逐步引导至奖励函数增大的决策空间。但是这种方法可能很难应用于复杂的问题中。</li>
<li>使用HER——事后经验池机制</li>
</ol>
<h2 id="her">HER</h2>
<blockquote>
<p>The pivotal idea behind our approach is to re-examine this trajectory with a different goal — while this trajectory may not help us learn how to achieve the state g, it definitely tells us something about how to achieve the state <span class="math inline">\(s_{T}\)</span> .</p>
</blockquote>
<p>HER的主要思想就是：<strong>为什么一定要考虑我们设定的目标呢？假设我们想让一个智能体学会移动到某个位置，它在一个episode中没有学到移动到目标位置就算失败吗？假定序列为 <span class="math inline">\(s_{0},s_{1},s_{2}, \cdots ,s_{T}\)</span> ，目标为<span class="math inline">\(g\)</span>，我们何不换一种思路考虑：如果我们在episode开始前就将目标状态 <span class="math inline">\(g\)</span> 设置为 <span class="math inline">\(s_{T}\)</span> ，即 <span class="math inline">\(g=s_{T}\)</span> ，那么这样看来智能体不就算是完成目标了吗？</strong></p>
<p><img src="/images/2019-06-01-Hindsight-Experience-Replay/Her.png" /></p>
<p>HER就是运用了这个思想对经验池进行了扩充，将稀疏奖励问题给转化成非稀疏奖励，大大的扩展了经验池中完成任务的经验数量。</p>
<p>HER主要特点：</p>
<ul>
<li>传统经验池存入的是状态 <span class="math inline">\(s​\)</span> ，而HER存入的是 <span class="math inline">\(s||g​\)</span> ，也就是 <code>tf.concat(s,g)</code></li>
<li>训练算法的输入也是 <span class="math inline">\(s||g\)</span> ，也就是需要在当前状态后边连结上<strong>每个episode的</strong>目标状态，每个episode的目标状态可能不同</li>
<li>HER对经验池进行了扩充，不仅存入实际采样得到的transition/experience， <span class="math inline">\(\left ( s_{t}||g,a_{t},r_{t},s_{t+1}||g \right )\)</span> ，也要在回合结束时<strong>重新设置目标状态</strong>，得到相应的奖励值（在二分奖励问题中，只有在 <span class="math inline">\(s=g\)</span> 时奖励才需要更改），存入“事后”（当初如果这样就好啦！）的经验 <span class="math inline">\(\left ( s_{t}||g&#39;,a_{t},r_{t}&#39;,s_{t+1}||g&#39; \right )\)</span> ，详见伪代码，这个事后经验究竟存入多少份、多少种，由超参数 <span class="math inline">\(k\)</span> 控制，下文讲解。</li>
<li>HER更适合解决多目标问题，多目标的意思为，目标点非固定，每个episode的目标状态可以不相同。详见实验部分</li>
</ul>
<p>HER的几种扩展方式：</p>
<blockquote>
<p>future — replay with k random states which come from the same episode as the transition being replayed and were observed after it, episode — replay with k random states coming from the same episode as the transition being replayed, random — replay with k random states encountered so far in the whole training procedure.</p>
</blockquote>
<ul>
<li><p>未来模式——future：在一个序列 <span class="math inline">\(s_{0},s_{1},s_{2},\cdots,s_{T}\)</span> 中，如果遍历到状态 <span class="math inline">\(s_{2}\)</span> ，则在<span class="math inline">\(s_{3},\cdots,s_{T}\)</span>之间随机抽取 <span class="math inline">\(k\)</span> 个状态作为目标状态 <span class="math inline">\(g&#39;\)</span> ，并依此向经验池中存入 <span class="math inline">\(\left ( s_{2}||g&#39;,a_{2},r_{2}&#39;,s_{3}||g&#39; \right )\)</span> ，<strong>特点：一个episode的后续部分</strong></p></li>
<li><p>回合模式——episode：在一个序列 <span class="math inline">\(s_{0},s_{1},s_{2},...,s_{T}\)</span> 中，如果遍历到状态 <span class="math inline">\(s_{2}\)</span> ，则在整个序列中随机抽取 <span class="math inline">\(k\)</span> 个状态作为目标状态 <span class="math inline">\(g&#39;\)</span> ，并依此向经验池中存入 <span class="math inline">\(\left ( s_{2}||g&#39;,a_{2},r_{2}&#39;,s_{3}||g&#39; \right )\)</span> ，<strong>特点：一个episode</strong></p></li>
<li><p>随机模式——random：在一个序列 <span class="math inline">\(s_{0},s_{1},s_{2},...,s_{T}\)</span> 中，如果遍历到状态 <span class="math inline">\(s_{2}\)</span> ，则在多个序列 <span class="math inline">\(\tau_{0},\tau_{1},\tau_{2},\cdots\)</span> 中随机抽取 <span class="math inline">\(k\)</span> 个状态作为目标状态 <span class="math inline">\(g&#39;\)</span> ，并依此向经验池中存入 <span class="math inline">\(\left ( s_{2}||g&#39;,a_{2},r_{2}&#39;,s_{3}||g&#39; \right )\)</span> ，<strong>特点：多个episode</strong></p></li>
<li><p>最终模式——final：在一个序列 <span class="math inline">\(s_{0},s_{1},s_{2},\cdots,s_{T}\)</span> 中，如果遍历到状态 <span class="math inline">\(s_{2}\)</span> ，则之间令 <span class="math inline">\(g&#39;=s_{T}\)</span> ，并向经验池中存 入<span class="math inline">\(\left ( s_{2}||g&#39;,a_{2},r_{2}&#39;,s_{3}||g&#39; \right )\)</span> ，<strong>特点：一个episode的最后一个状态，如果设置k，则存入k个相同的经验</strong></p></li>
</ul>
<h2 id="伪代码">伪代码</h2>
<p><img src="/images/2019-06-01-Hindsight-Experience-Replay/pseudo.png" /></p>
<p>解析：</p>
<ol type="1">
<li>伪代码中没有提到超参数 <span class="math inline">\(k\)</span> ，其实在循环条件 <span class="math inline">\(\textbf{for} \ g&#39; \in G \ \textbf{do}\)</span> 中循环执行了 <span class="math inline">\(k\)</span> 次</li>
<li><span class="math inline">\(||\)</span> 操作为连结操作，简言之，将两个长度为5的向量合并成一个长度为10的向量</li>
<li><span class="math inline">\(G:=\mathbb{S}(\textbf{current episode})\)</span> 即为上文提到的四种扩展模式：future、episode、random、final。</li>
<li>奖励函数 <span class="math inline">\(r(s,a,g)=-\left [ f_{g}(s)=0 \right ]\)</span> 即为前文提到的 <span class="math inline">\(r_{g}(s,a)=-\left [ s \neq g \right ]\)</span> ，即完成为0，未完成为-1，具体奖励函数可以根据我们的使用环境设计</li>
<li><span class="math inline">\(a_{t} \leftarrow \pi_{b}(s_{t}||g)\)</span> 表示神经网络的输入为当前状态与目标状态的连结</li>
</ol>
<h2 id="her的优点">HER的优点</h2>
<ol type="1">
<li>可解决稀疏奖励、二分奖励问题</li>
<li>可适用于所有的Off-Policy算法</li>
<li>提升了数据采样效率</li>
</ol>
<h1 id="实验部分">实验部分</h1>
<p>文中实验结果：<a href="https://goo.gl/SMrQnI" target="_blank" rel="noopener">https://goo.gl/SMrQnI</a></p>
<p>实验部分的完整细节请参考论文原文。</p>
<h2 id="环境">环境</h2>
<ul>
<li>7自由度机械臂</li>
<li>模拟环境：MuJoCo</li>
<li>任务分为3种
<ul>
<li>Pushing，推：锁定机械臂的钳子，移动机械臂将物体推到目标点</li>
<li>Sliding，滑动：类似于冰球运动，锁定机械臂的钳子，移动机械臂给与物体一个力，使物体可以在较光滑的桌面上滑动并且达到目标位置</li>
<li>Pick-and-place，摆放：解锁钳子，使用机械臂夹起物体并移动至空中目标点</li>
</ul></li>
</ul>
<p><img src="/images/2019-06-01-Hindsight-Experience-Replay/tasks.png" /></p>
<h2 id="算法">算法</h2>
<ul>
<li>DDPG</li>
<li>Adam优化器</li>
<li>多层感知机MLPs</li>
<li>ReLU激活函数</li>
<li>8核并行，更新参数后取平均</li>
<li>A-C网络都是3个隐藏层，每层64个隐节点，Actor输出层用tanh激活函数</li>
<li>经验池大小为 <span class="math inline">\(10^{6}\)</span>，折扣因子<span class="math inline">\(\gamma=0.98\)</span> ，学习率 <span class="math inline">\(\alpha=0.001\)</span> ，探索因子 <span class="math inline">\(\epsilon = 0.2\)</span></li>
</ul>
<blockquote>
<p>With probability 20% we sample (uniformly) a random action from the hypercube of valid actions.</p>
</blockquote>
<p>DDPG使用了随机探索机制</p>
<h2 id="训练结果">训练结果</h2>
<h3 id="final模式与future模式对比">final模式与future模式对比</h3>
<p><img src="/images/2019-06-01-Hindsight-Experience-Replay/finalvsfuture.png" /></p>
<ul>
<li>红色曲线为future模式，蓝色曲线为final模式，绿色曲线为使用了<a href="https://arxiv.org/pdf/1703.01310.pdf" target="_blank" rel="noopener">count-based</a> 的DDPG，褐红色虚线为原始DDPG</li>
<li>从左至右依次是Pushing，Sliding，Pick-and-place任务</li>
<li>超参数 <span class="math inline">\(k=4\)</span></li>
<li>这个实验中，目标状态会变，即为多个目标状态</li>
</ul>
<p>结果分析：</p>
<ul>
<li>future模式比final效果更好</li>
<li>使用了count-based的DDPG智能稍微解决一下Sliding任务</li>
<li>使用HER的DDPG可以完全胜任三个任务</li>
<li>证明了HER是使从稀疏、二分奖励问题中学习成为可能的关键因素</li>
</ul>
<h3 id="单个目标状态的实验">单个目标状态的实验</h3>
<p><img src="/images/2019-06-01-Hindsight-Experience-Replay/singlegoal.png" /></p>
<ul>
<li>蓝色曲线为使用了 HER 的 DDPG，文中并未说明 HER 是哪种模式，<strong>猜测</strong>是 final 模式，因为文中实验部分之前都是以 final 模式进行举例</li>
<li>绿色曲线代表应用了 count-based 的DDPG，褐红色虚线为原始DDPG</li>
<li>实验中，目标状态都为同一状态<span class="math inline">\(g\)</span></li>
</ul>
<p>结果分析：</p>
<ul>
<li>DDPG+HER 比原始 DDPG 的性能要好很多</li>
<li><strong>相比于多个目标的实验，可以发现，在多目标的任务中DDPG训练更快</strong>，所以在实际中，即使我们只关心一个目标，我们最好也使用多个目标来训练</li>
</ul>
<h3 id="her应用于reward-shaping问题中">HER应用于reward shaping问题中</h3>
<p>前文已经说过，reward shaping可以简单理解为将奖励函数设置为某些变量的函数，如 <span class="math inline">\(r_{g}(s,a)=-\left || s-g \right ||^{2}\)</span> ，即奖励函数为当前状态与目标状态的欧氏距离的负数</p>
<p><img src="/images/2019-06-01-Hindsight-Experience-Replay/rewardshape.png" /></p>
<ul>
<li>奖励函数为 <span class="math inline">\(r_{g}(s,a)=-\left || s-g \right ||^{2}​\)</span></li>
</ul>
<p>结果分析：</p>
<ul>
<li><p>无论使用怎样的reward shaping函数，DDPG、DDPG+HER都不能解决这个问题</p></li>
<li><p>作者认为原因有二：</p>
<ul>
<li><blockquote>
<p>There is a huge discrepancy between what we optimize (i.e. a shaped reward function) and the success condition (i.e.: is the object within some radius from the goal at the end of the episode);</p>
</blockquote>
<p>判定完成目标的条件和要优化的问题有巨大的矛盾（虽然我也不理解这到底是什么意思，索性就直接抄了过来）</p></li>
<li><blockquote>
<p>Shaped rewards penalize for inappropriate behaviour (e.g. moving the box in a wrong direction) which may hinder exploration. It can cause the agent to learn not to touch the box at all if it can not manipulate it precisely and we noticed such behaviour in some of our experiments.</p>
</blockquote>
<p>reward shaping阻碍了探索</p></li>
</ul></li>
<li><blockquote>
<p>Our results suggest that domain-agnostic reward shaping does not work well (at least in the simple forms we have tried). Of course for every problem there exists a reward which makes it easy (Ng et al., 1999) but designing such shaped rewards requires a lot of domain knowledge and may in some cases not be much easier than directly scripting the policy. This strengthens our belief that learning from sparse, binary rewards is an important problem.</p>
</blockquote>
<p>研究结果表明，与领域无关的reward shaping效果并不好</p></li>
</ul>
<h3 id="四种模式比较">四种模式比较</h3>
<p><img src="/images/2019-06-01-Hindsight-Experience-Replay/fourmodel.png" /></p>
<ul>
<li>红色代表future模式，蓝色代表final模式，绿色代表episode模式，紫色代表episode模式，褐红色虚线代表原始DDPG</li>
<li>横坐标代表超参数 <span class="math inline">\(k\)</span> ，第一行三个图的纵坐标代表最高得分，第二行三个图的纵坐标代表平均得分</li>
</ul>
<p>结果分析：</p>
<ul>
<li><p>效果：future&gt;final&gt;episode&gt;random&gt;no HER</p></li>
<li><p>稳定性：final(好)=no-HER(差)&gt;future&gt;episode&gt;random</p></li>
<li><p>future模式是唯一一个可以解决Sliding任务的，在 <span class="math inline">\(k=4\)</span> 或者 <span class="math inline">\(k=8\)</span> 时效果最好</p></li>
<li><p>增大<span class="math inline">\(k\)</span>超过8会使性能有所下降，主要是因为<span class="math inline">\(k\)</span>过大导致经验池中原始真实数据所占的比例太小</p></li>
<li><blockquote>
<p>It confirms that the most valuable goals for replay are the ones which are going to be achieved in the near future</p>
</blockquote>
<p>它证实了回放经验中最有价值的目标是那些在不久的将来能实现的目标</p></li>
</ul>
<p><em>注：作者根据 future 模式提出了最近邻的 future 模式，即把 <span class="math inline">\(g&#39;\)</span> 设置为 <span class="math inline">\(s_{t+1}\)</span> ，并且进行了实验，实验结果不如 future 模式。</em></p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/wechatpay.jpg" alt="Fisher Chang 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/alipay.jpg" alt="Fisher Chang 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Fisher Chang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://bluefisher.github.io/2019/06/01/Hindsight-Experience-Replay/" title="Hindsight Experience Replay">https://bluefisher.github.io/2019/06/01/Hindsight-Experience-Replay/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/rl/" rel="tag"># rl</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/03/22/Soft-Actor-Critic/" rel="prev" title="Soft Actor-Critic">
      <i class="fa fa-chevron-left"></i> Soft Actor-Critic
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/06/12/Diversity-is-All-You-Need-Learning-Skills-without-a-Reward-Function/" rel="next" title="Diversity is All You Need: Learning Skills without a Reward Function">
      Diversity is All You Need: Learning Skills without a Reward Function <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E5%88%86%E5%A5%96%E5%8A%B1-binary-reward"><span class="nav-number">1.1.</span> <span class="nav-text">二分奖励 binary reward</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A8%80%E7%96%8F%E5%A5%96%E5%8A%B1-sparse-reward"><span class="nav-number">1.2.</span> <span class="nav-text">稀疏奖励 sparse reward</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%96%87%E4%B8%AD%E7%B2%BE%E8%A6%81"><span class="nav-number">2.</span> <span class="nav-text">文中精要</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#her"><span class="nav-number">2.1.</span> <span class="nav-text">HER</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%AA%E4%BB%A3%E7%A0%81"><span class="nav-number">2.2.</span> <span class="nav-text">伪代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#her%E7%9A%84%E4%BC%98%E7%82%B9"><span class="nav-number">2.3.</span> <span class="nav-text">HER的优点</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E9%83%A8%E5%88%86"><span class="nav-number">3.</span> <span class="nav-text">实验部分</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83"><span class="nav-number">3.1.</span> <span class="nav-text">环境</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95"><span class="nav-number">3.2.</span> <span class="nav-text">算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C"><span class="nav-number">3.3.</span> <span class="nav-text">训练结果</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#final%E6%A8%A1%E5%BC%8F%E4%B8%8Efuture%E6%A8%A1%E5%BC%8F%E5%AF%B9%E6%AF%94"><span class="nav-number">3.3.1.</span> <span class="nav-text">final模式与future模式对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E4%B8%AA%E7%9B%AE%E6%A0%87%E7%8A%B6%E6%80%81%E7%9A%84%E5%AE%9E%E9%AA%8C"><span class="nav-number">3.3.2.</span> <span class="nav-text">单个目标状态的实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#her%E5%BA%94%E7%94%A8%E4%BA%8Ereward-shaping%E9%97%AE%E9%A2%98%E4%B8%AD"><span class="nav-number">3.3.3.</span> <span class="nav-text">HER应用于reward shaping问题中</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E7%A7%8D%E6%A8%A1%E5%BC%8F%E6%AF%94%E8%BE%83"><span class="nav-number">3.3.4.</span> <span class="nav-text">四种模式比较</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Fisher Chang"
      src="https://avatars0.githubusercontent.com/u/6104460?v=3&s=200">
  <p class="site-author-name" itemprop="name">Fisher Chang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">63</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/BlueFisher" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;BlueFisher" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/4909165/fish-tree" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;4909165&#x2F;fish-tree" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://ai.stackexchange.com/users/15525/fish-tree" title="AI.StackExchange → https:&#x2F;&#x2F;ai.stackexchange.com&#x2F;users&#x2F;15525&#x2F;fish-tree" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-exchange"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:blue.fisher.zzy@gmail.com" title="E-Mail → mailto:blue.fisher.zzy@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://stepneverstop.github.io/" title="https:&#x2F;&#x2F;stepneverstop.github.io&#x2F;" rel="noopener" target="_blank">Keavnn</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2016 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fisher Chang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : '7ruImtbdRTEUvc3qxbGlSc0t-gzGzoHsz',
      appKey     : 'Vbun7eF790Szhvl9qKC0CCsm',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
